{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9aa8e0-72d7-4c1b-83e6-6d4770793d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "844a2cba-32f4-4ad6-b6ef-bab2246a27e9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:06:21.132228Z",
     "iopub.status.busy": "2024-07-09T10:06:21.131998Z",
     "iopub.status.idle": "2024-07-09T10:06:38.357846Z",
     "shell.execute_reply": "2024-07-09T10:06:38.357238Z",
     "shell.execute_reply.started": "2024-07-09T10:06:21.132212Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "documents = []\n",
    "dc_name = [\"soybean_konw.pdf\", \"soybean2.pdf\"]\n",
    "for tmp_name in dc_name:\n",
    "    # print(len(PyPDFLoader(tmp_name).load()))\n",
    "    documents += PyPDFLoader(tmp_name).load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(documents[:])\n",
    "for idx, text in enumerate(texts):\n",
    "    text.metadata[\"id\"] = idx\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "top_k = 10\n",
    "\n",
    "model_name = '/mnt/workspace/.cache/modelscope/hub/maple77/zpoint_large_embedding_zh'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "vectorstore = Chroma(persist_directory=\"soybean_db2\", embedding_function=hf)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": top_k}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e851a431-7fb0-434f-90c5-016fd5a5cb93",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:06:38.359523Z",
     "iopub.status.busy": "2024-07-09T10:06:38.359057Z",
     "iopub.status.idle": "2024-07-09T10:06:38.537341Z",
     "shell.execute_reply": "2024-07-09T10:06:38.536757Z",
     "shell.execute_reply.started": "2024-07-09T10:06:38.359504Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List\n",
    "from typing import Literal, Optional, Tuple\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        ...,\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.6,\n",
    "    model=\"glm-4-0520\",\n",
    "    openai_api_key=\"661a7aa0aeb8ca129eb4647461123230.bl9w581QKpnMfBvs\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "# structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "# Set up a parser\n",
    "parser = PydanticOutputParser(pydantic_object=GradeDocuments)\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.7,\n",
    "    model=\"glm-4-0520\",\n",
    "    openai_api_key=\"661a7aa0aeb8ca129eb4647461123230.bl9w581QKpnMfBvs\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | llm | parser\n",
    "# question = \"agent memory\"\n",
    "# docs = retriever.invoke(question)\n",
    "# doc_txt = texts[3].page_content\n",
    "# print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131ec65e-c097-4d78-9b28-9e89476d55a5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:06:38.538290Z",
     "iopub.status.busy": "2024-07-09T10:06:38.538032Z",
     "iopub.status.idle": "2024-07-09T10:06:38.646703Z",
     "shell.execute_reply": "2024-07-09T10:06:38.646044Z",
     "shell.execute_reply.started": "2024-07-09T10:06:38.538265Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Generate\n",
    "from langchain import PromptTemplate\n",
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "# Prompt\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "template = \"\"\"\n",
    "You are an expert in phenomics in agronomy, and you have a very rich knowledge of agronomy and phenomics.\n",
    "Use the context snippets retrieved below to answer the agronomy question from a phenotypic point of view.\n",
    "If if you don't know the answer, say you don't know.\n",
    "\n",
    "Question: {question} \n",
    "\n",
    "Context: {context} \n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template, \n",
    "    input_variables=[\"context\",\"question\"]\n",
    "  )\n",
    "# LLM\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.6,\n",
    "    model=\"glm-4-0520\",\n",
    "    openai_api_key=\"661a7aa0aeb8ca129eb4647461123230.bl9w581QKpnMfBvs\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "# rag_chain = prompt | llm | StrOutputParser()\n",
    "rag_chain = (\n",
    "    {\"context\": RunnablePassthrough(),  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    ")\n",
    "# Run\n",
    "# generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "# print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30cc0c23-7417-44bb-9cf0-4c3149f2ace0",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:06:48.180104Z",
     "iopub.status.busy": "2024-07-09T10:06:48.179757Z",
     "iopub.status.idle": "2024-07-09T10:06:48.270394Z",
     "shell.execute_reply": "2024-07-09T10:06:48.269850Z",
     "shell.execute_reply.started": "2024-07-09T10:06:48.180085Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Hallucination Grader\n",
    "from typing import List\n",
    "from typing import Literal, Optional, Tuple\n",
    "\n",
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        ...,\n",
    "        description=\"Answer is grounded in the facts, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "parser2 = PydanticOutputParser(pydantic_object=GradeHallucinations)\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.6,\n",
    "    model=\"glm-4-0520\",\n",
    "    openai_api_key=\"661a7aa0aeb8ca129eb4647461123230.bl9w581QKpnMfBvs\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "# structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts. \\n \n",
    "     Give a binary score 'yes' or 'no'. 'Yes' means that the answer is grounded in / supported by the set of facts.\n",
    "     Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\"\"\"\n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "hallucination_grader = hallucination_prompt | llm | parser2\n",
    "# hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82f61967-9e51-4dac-960a-44f562c64458",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:06:54.253553Z",
     "iopub.status.busy": "2024-07-09T10:06:54.253191Z",
     "iopub.status.idle": "2024-07-09T10:06:54.345534Z",
     "shell.execute_reply": "2024-07-09T10:06:54.344948Z",
     "shell.execute_reply.started": "2024-07-09T10:06:54.253528Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: Literal[\"yes\", \"no\"] = Field(\n",
    "        ...,\n",
    "        description=\"Answer addresses the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "parser3 = PydanticOutputParser(pydantic_object=GradeHallucinations)\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.6,\n",
    "    model=\"glm-4-0520\",\n",
    "    openai_api_key=\"661a7aa0aeb8ca129eb4647461123230.bl9w581QKpnMfBvs\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing whether an answer addresses / resolves a question \\n \n",
    "     Give a binary score 'yes' or 'no'. Yes' means that the answer resolves the question.\\n\n",
    "     Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\"\"\"\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"User question: \\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "answer_grader = answer_prompt | llm | parser3\n",
    "# answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea57f31-1f7b-464c-a6a7-995fd471fffa",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:06:56.125092Z",
     "iopub.status.busy": "2024-07-09T10:06:56.124645Z",
     "iopub.status.idle": "2024-07-09T10:06:56.224349Z",
     "shell.execute_reply": "2024-07-09T10:06:56.223793Z",
     "shell.execute_reply.started": "2024-07-09T10:06:56.125056Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Question Re-writer\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.6,\n",
    "    model=\"glm-4-0520\",\n",
    "    openai_api_key=\"661a7aa0aeb8ca129eb4647461123230.bl9w581QKpnMfBvs\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "# question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3da7eca-b53b-4c61-8787-290672a50bf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T10:06:57.904233Z",
     "iopub.status.busy": "2024-07-09T10:06:57.903824Z",
     "iopub.status.idle": "2024-07-09T10:06:57.908118Z",
     "shell.execute_reply": "2024-07-09T10:06:57.907567Z",
     "shell.execute_reply.started": "2024-07-09T10:06:57.904204Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "babec9f0-99db-4a3e-a7e5-63961a97ebc8",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:07:01.277857Z",
     "iopub.status.busy": "2024-07-09T10:07:01.277526Z",
     "iopub.status.idle": "2024-07-09T10:07:01.281131Z",
     "shell.execute_reply": "2024-07-09T10:07:01.280533Z",
     "shell.execute_reply.started": "2024-07-09T10:07:01.277838Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Nodes\n",
    "\n",
    "\n",
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---检索文档中---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2bb2f3f-41ef-49dd-8855-caeb6e29a663",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:07:03.205601Z",
     "iopub.status.busy": "2024-07-09T10:07:03.205243Z",
     "iopub.status.idle": "2024-07-09T10:07:03.209348Z",
     "shell.execute_reply": "2024-07-09T10:07:03.208830Z",
     "shell.execute_reply.started": "2024-07-09T10:07:03.205583Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---正在根据检索结果 生成回答---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    dok_tmp = ''.join([dk.page_content for dk in documents])\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a222537-bfc9-46b3-af18-b0dd746cb9bc",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:07:05.221008Z",
     "iopub.status.busy": "2024-07-09T10:07:05.220594Z",
     "iopub.status.idle": "2024-07-09T10:07:05.226580Z",
     "shell.execute_reply": "2024-07-09T10:07:05.225954Z",
     "shell.execute_reply.started": "2024-07-09T10:07:05.220980Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---检查该问题是否与检索文档相关---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: 该文档与结果相关---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: 该文档与结果不相关---\")\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"question\": question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15d54f9c-6c90-47ad-965a-f58429ea80f5",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:07:06.990077Z",
     "iopub.status.busy": "2024-07-09T10:07:06.989645Z",
     "iopub.status.idle": "2024-07-09T10:07:06.994373Z",
     "shell.execute_reply": "2024-07-09T10:07:06.993766Z",
     "shell.execute_reply.started": "2024-07-09T10:07:06.990044Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---将问题进行重写一个新的问题---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f7b0802-9d4e-4748-9572-1d2db85a0c3d",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:07:08.700732Z",
     "iopub.status.busy": "2024-07-09T10:07:08.700402Z",
     "iopub.status.idle": "2024-07-09T10:07:08.704262Z",
     "shell.execute_reply": "2024-07-09T10:07:08.703749Z",
     "shell.execute_reply.started": "2024-07-09T10:07:08.700714Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: 所有文档不相干，重写问题---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION:文档相关，生成新的答案---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "092d0fad-2f29-49d5-95e5-724c4b7116ca",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:07:10.669535Z",
     "iopub.status.busy": "2024-07-09T10:07:10.669088Z",
     "iopub.status.idle": "2024-07-09T10:07:10.675715Z",
     "shell.execute_reply": "2024-07-09T10:07:10.675223Z",
     "shell.execute_reply.started": "2024-07-09T10:07:10.669502Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "def grade_generation_v_documents_and_question(state):\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---检查是否存在幻觉---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    grade = score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if grade == \"yes\":\n",
    "        print(\"---DECISION: 生成的回答是基于文档的---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---DECISION: 生成回答解决得来问题---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: 生成回答不解决问题---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        pprint(\"---DECISION: 生成回答不以文档为基础，重写问题---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "680b085e-6bb0-4ce6-9b12-6af7340f1f46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T10:07:12.481347Z",
     "iopub.status.busy": "2024-07-09T10:07:12.480991Z",
     "iopub.status.idle": "2024-07-09T10:07:12.510698Z",
     "shell.execute_reply": "2024-07-09T10:07:12.510106Z",
     "shell.execute_reply.started": "2024-07-09T10:07:12.481325Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_documents_and_question,\n",
    "    {\n",
    "        \"not supported\": \"generate\",\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"transform_query\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aa2ddd6-08a4-4924-bf6b-d443dcc165b9",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:07:14.253302Z",
     "iopub.status.busy": "2024-07-09T10:07:14.252960Z",
     "iopub.status.idle": "2024-07-09T10:07:48.538539Z",
     "shell.execute_reply": "2024-07-09T10:07:48.537702Z",
     "shell.execute_reply.started": "2024-07-09T10:07:14.253271Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---检索文档中---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---检查该问题是否与检索文档相关---\n",
      "---GRADE: 该文档与结果相关---\n",
      "---GRADE: 该文档与结果相关---\n",
      "---GRADE: 该文档与结果相关---\n",
      "---GRADE: 该文档与结果相关---\n",
      "---GRADE: 该文档与结果不相关---\n",
      "---GRADE: 该文档与结果不相关---\n",
      "---GRADE: 该文档与结果相关---\n",
      "---GRADE: 该文档与结果不相关---\n",
      "---GRADE: 该文档与结果不相关---\n",
      "---GRADE: 该文档与结果不相关---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION:文档相关，生成新的答案---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---正在根据检索结果 生成回答---\n",
      "---检查是否存在幻觉---\n",
      "---DECISION: 生成的回答是基于文档的---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: 生成回答解决得来问题---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('High-yielding soybeans, from a phenotypic standpoint, exhibit several '\n",
      " 'characteristics that contribute to their superior yield. Based on the '\n",
      " 'context provided:\\n'\n",
      " '\\n'\n",
      " '1. **Growth Rate**: High-yielding soybeans are likely to have an optimal '\n",
      " \"growth rate, which is critical during the crop's development phases, \"\n",
      " 'especially during the critical period for seed number determination. This '\n",
      " 'period is influenced by crop growth rate, duration, and dry matter '\n",
      " 'accumulation (Monzon et al., 2021).\\n'\n",
      " '\\n'\n",
      " '2. **Stress Tolerance**: These soybeans tend to have better tolerance to '\n",
      " 'stress, particularly during the reproductive phase, as stress at this stage '\n",
      " 'can be highly detrimental to yield, leading to significant yield loss.\\n'\n",
      " '\\n'\n",
      " '3. **Plant Architecture**: Newer high-yielding cultivars tend to have '\n",
      " 'shorter plant heights, which is associated with reduced lodging. However, '\n",
      " 'simply reducing plant height is not sufficient to directly increase yield '\n",
      " \"due to soybean's growth characteristics, such as pod formation at multiple \"\n",
      " 'nodes.\\n'\n",
      " '\\n'\n",
      " '4. **Developmental Phases**: The timing and duration of the vegetative '\n",
      " 'growth phase and the reproductive phase, which includes the seed formation '\n",
      " 'and seed filling periods, are critical in determining the final yield. '\n",
      " 'High-yielding soybeans likely have well-coordinated phases that contribute '\n",
      " 'positively to yield formation through various physiological processes.\\n'\n",
      " '\\n'\n",
      " '5. **Yield Components**: Characteristics such as the number of seeds per '\n",
      " 'unit land area and the weight per seed are important components that '\n",
      " 'contribute to overall yield. High-yielding soybeans would typically exhibit '\n",
      " 'an optimal balance of these components.\\n'\n",
      " '\\n'\n",
      " '6. **Canopy Phenotype**: The context suggests that improvements in soybean '\n",
      " 'breeding for higher yields can be enhanced by high-throughput canopy '\n",
      " 'phenotyping, indicating that canopy characteristics (like structure, size, '\n",
      " 'and function) are also important phenotypic traits of high-yielding '\n",
      " 'soybeans.\\n'\n",
      " '\\n'\n",
      " 'These characteristics collectively contribute to the phenotypic profile of '\n",
      " 'high-yielding soybean varieties, influencing their ability to produce '\n",
      " 'superior yields under varying conditions.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"What are the characteristics of high-yielding soybeans phenotypically, such as growth rate? Answer as much as you can.\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4b83bb2c-5af1-451f-9d8c-ae9842aa188a",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-09T10:40:13.759730Z",
     "iopub.status.busy": "2024-07-09T10:40:13.759410Z",
     "iopub.status.idle": "2024-07-09T10:40:13.762706Z",
     "shell.execute_reply": "2024-07-09T10:40:13.762165Z",
     "shell.execute_reply.started": "2024-07-09T10:40:13.759711Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-yielding soybeans, from a phenotypic standpoint, exhibit several characteristics that contribute to their superior yield. Based on the context provided:\n",
      "\n",
      "1. **Growth Rate**: High-yielding soybeans are likely to have an optimal growth rate, which is critical during the crop's development phases, especially during the critical period for seed number determination. This period is influenced by crop growth rate, duration, and dry matter accumulation (Monzon et al., 2021).\n",
      "\n",
      "2. **Stress Tolerance**: These soybeans tend to have better tolerance to stress, particularly during the reproductive phase, as stress at this stage can be highly detrimental to yield, leading to significant yield loss.\n",
      "\n",
      "3. **Plant Architecture**: Newer high-yielding cultivars tend to have shorter plant heights, which is associated with reduced lodging. However, simply reducing plant height is not sufficient to directly increase yield due to soybean's growth characteristics, such as pod formation at multiple nodes.\n",
      "\n",
      "4. **Developmental Phases**: The timing and duration of the vegetative growth phase and the reproductive phase, which includes the seed formation and seed filling periods, are critical in determining the final yield. High-yielding soybeans likely have well-coordinated phases that contribute positively to yield formation through various physiological processes.\n",
      "\n",
      "5. **Yield Components**: Characteristics such as the number of seeds per unit land area and the weight per seed are important components that contribute to overall yield. High-yielding soybeans would typically exhibit an optimal balance of these components.\n",
      "\n",
      "6. **Canopy Phenotype**: The context suggests that improvements in soybean breeding for higher yields can be enhanced by high-throughput canopy phenotyping, indicating that canopy characteristics (like structure, size, and function) are also important phenotypic traits of high-yielding soybeans.\n",
      "\n",
      "These characteristics collectively contribute to the phenotypic profile of high-yielding soybean varieties, influencing their ability to produce superior yields under varying conditions.\n"
     ]
    }
   ],
   "source": [
    "print(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
