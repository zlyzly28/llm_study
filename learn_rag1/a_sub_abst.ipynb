{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e507c4-7633-46ea-afa3-37e84a16abb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T02:30:32.889280Z",
     "iopub.status.busy": "2024-07-12T02:30:32.888934Z",
     "iopub.status.idle": "2024-07-12T02:30:32.988711Z",
     "shell.execute_reply": "2024-07-12T02:30:32.988170Z",
     "shell.execute_reply.started": "2024-07-12T02:30:32.889262Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import List\n",
    "from typing import Literal, Optional, Tuple\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        loads(doc)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "class Soybean_Q_3(BaseModel):\n",
    "\n",
    "    question1: str = Field(\n",
    "        ..., description=\"Generate one question based only on the following queries.The generated question is a rewrite of the question, which is the more abstract step-back question.(Question1)\"\n",
    "    )\n",
    "    question2: str = Field(\n",
    "        ..., description=\"Generate one question based only on the following queries.The generated question is a rewrite of the question, which is a more concrete sub-question.(Question2)\"\n",
    "    )\n",
    "    question3: str = Field(\n",
    "        ..., description=\" Generate one question based only on the following queries.The generated question is a rewrite of the question at the same level as the question, and is neither more abstract nor more concrete.(Question3)\"\n",
    "    )\n",
    "\n",
    "# Set up a parser\n",
    "parser = PydanticOutputParser(pydantic_object=Soybean_Q_3)\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\", \n",
    "            \"\"\"\n",
    "            You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "            Generate multiple search queries related to {query}, which are a more abstract step-back question, a more concrete sub-question, and a rewrite of a pair of questions.\n",
    "            \"\"\"\n",
    "        ),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.6,\n",
    "    model=\"glm-4-0520\",\n",
    "    openai_api_key=\"661a7aa0aeb8ca129eb4647461123230.bl9w581QKpnMfBvs\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "rag_chain = (\n",
    "    {\"query\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | parser \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253ca560-4de9-47a9-b486-3c4e0b16df7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T09:20:39.535474Z",
     "iopub.status.busy": "2024-07-05T09:20:39.535155Z",
     "iopub.status.idle": "2024-07-05T09:20:59.108078Z",
     "shell.execute_reply": "2024-07-05T09:20:59.107374Z",
     "shell.execute_reply.started": "2024-07-05T09:20:39.535456Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'source', 'page', 'question', 'ground_truth', 'context'])\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "from retiever_eval_list import get_result_retrieva, get_retriever_res_list\n",
    "import os\n",
    "import shutil\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "df = pd.read_excel('soybean_q_gt_609.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# 修改\n",
    "folder_path = 'Result/a_sub_abst/'\n",
    "retriever_filename = \"Result/a_sub_abst/retriever_result.json\"\n",
    "save_info_result_filename = \"Result/a_sub_abst/save_info_result.json\"\n",
    "top_k = 10\n",
    "s_index = 3\n",
    "\n",
    "model_name = '/mnt/workspace/.cache/modelscope/hub/maple77/zpoint_large_embedding_zh'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "vectorstore = Chroma(persist_directory=\"soybean_db2\", embedding_function=hf)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": top_k}\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/mnt/workspace/.cache/modelscope/hub/Xorbits/bge-reranker-base')\n",
    "rerank_model = AutoModelForSequenceClassification.from_pretrained('/mnt/workspace/.cache/modelscope/hub/Xorbits/bge-reranker-base')\n",
    "\n",
    "\n",
    "def create_folder_if_not_exists(folder_path):\n",
    "    # 检查文件夹是否存在\n",
    "    if os.path.exists(folder_path):\n",
    "        # 如果存在，则删除原文件夹及其中内容\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"Folder '{folder_path}' existed and has been removed.\")\n",
    "    # 创建新文件夹\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder '{folder_path}' has been created.\")\n",
    "\n",
    "# 打印DataFrame的内容\n",
    "column_lists = {col: df[col].tolist() for col in df.columns}\n",
    "print(column_lists.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab76c063-275f-4713-9380-df73b3ed3370",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T09:21:00.916041Z",
     "iopub.status.busy": "2024-07-05T09:21:00.915529Z",
     "iopub.status.idle": "2024-07-05T09:22:17.684792Z",
     "shell.execute_reply": "2024-07-05T09:22:17.684236Z",
     "shell.execute_reply.started": "2024-07-05T09:21:00.916018Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get retriever result: 100%|██████████| 3/3 [01:16<00:00, 25.59s/it]\n"
     ]
    }
   ],
   "source": [
    "retriever_result = []\n",
    "for tmp_q in tqdm(range(len(column_lists['question'][:s_index])), desc='Get retriever result'):\n",
    "    # print(tmp_q)\n",
    "    main_query = column_lists['question'][tmp_q]\n",
    "    sub_query = rag_chain.invoke({\"query\": main_query})\n",
    "    retriever_result.append([retriever.invoke(main_query) , retriever.invoke(sub_query.question1) , retriever.invoke(sub_query.question2) , retriever.invoke(sub_query.question3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92e80eed-b8d6-454a-9a36-719c5f92dc08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T09:23:18.011502Z",
     "iopub.status.busy": "2024-07-05T09:23:18.011047Z",
     "iopub.status.idle": "2024-07-05T09:23:18.021328Z",
     "shell.execute_reply": "2024-07-05T09:23:18.020711Z",
     "shell.execute_reply.started": "2024-07-05T09:23:18.011469Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reciprocal_result = []\n",
    "for tmp in retriever_result:\n",
    "    reciprocal_result.append(reciprocal_rank_fusion(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33a0c307-0a4e-4904-9267-06d00da487e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T09:23:20.611530Z",
     "iopub.status.busy": "2024-07-05T09:23:20.611206Z",
     "iopub.status.idle": "2024-07-05T09:23:20.616025Z",
     "shell.execute_reply": "2024-07-05T09:23:20.615507Z",
     "shell.execute_reply.started": "2024-07-05T09:23:20.611510Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'Result/a_sub_abst/' has been created.\n",
      "完成!\n"
     ]
    }
   ],
   "source": [
    "col_id = column_lists['id'][:s_index]\n",
    "rerank_result = get_result_retrieva(col_id, reciprocal_result, topk=top_k)\n",
    "# 用你想要的路径替换'your_folder_path'\n",
    "create_folder_if_not_exists(folder_path)\n",
    "# 对结果进行保存\n",
    "# 指定你想要保存的文件名\n",
    "# 使用json.dump()将字典保存为json文件\n",
    "with open(retriever_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(rerank_result, f, ensure_ascii=False, indent=4)\n",
    "print('完成!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "029a488d-4dd7-4fcf-9c65-0a5a7d49b0dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T09:23:22.690702Z",
     "iopub.status.busy": "2024-07-05T09:23:22.690329Z",
     "iopub.status.idle": "2024-07-05T09:23:22.695383Z",
     "shell.execute_reply": "2024-07-05T09:23:22.694897Z",
     "shell.execute_reply.started": "2024-07-05T09:23:22.690678Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成!\n"
     ]
    }
   ],
   "source": [
    "# dict_keys(['id', 'source', 'page', 'question', 'ground_truth', 'context'])\n",
    "# column_lists['context'][:s_index]\n",
    "save_info_result = {}\n",
    "save_info_result['id'] = column_lists['id'][:s_index]\n",
    "save_info_result['source'] = column_lists['source'][:s_index]\n",
    "save_info_result['page'] = column_lists['page'][:s_index]\n",
    "save_info_result['question'] = column_lists['question'][:s_index]\n",
    "save_info_result['ground_truth'] = column_lists['ground_truth'][:s_index]\n",
    "save_info_result['context'] = column_lists['context'][:s_index]\n",
    "save_info_result['retriever_result_list'] = get_retriever_res_list(reciprocal_result, top_k)\n",
    "\n",
    "# 使用json.dump()将字典保存为json文件\n",
    "with open(save_info_result_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(save_info_result, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print('完成!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
