{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d6bf012-ae5b-4909-803a-a2494b17e670",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-05T08:03:10.018838Z",
     "iopub.status.busy": "2024-07-05T08:03:10.018350Z",
     "iopub.status.idle": "2024-07-05T08:03:10.757268Z",
     "shell.execute_reply": "2024-07-05T08:03:10.756622Z",
     "shell.execute_reply.started": "2024-07-05T08:03:10.018819Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import List\n",
    "from typing import Literal, Optional, Tuple\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        loads(doc)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "class Soybean_Q_3(BaseModel):\n",
    "\n",
    "    question1: str = Field(\n",
    "        ..., description=\"Given contextual information, not prior knowledge. Generate one question based only on the following queries.(Question1)\"\n",
    "    )\n",
    "    question2: str = Field(\n",
    "        ..., description=\"Given contextual information, not prior knowledge. Generate one question based only on the following queries.(Question2)\"\n",
    "    )\n",
    "    question3: str = Field(\n",
    "        ..., description=\"Given contextual information, not prior knowledge. Generate one question based only on the following queries.(Question3)\"\n",
    "    )\n",
    "\n",
    "# Set up a parser\n",
    "parser = PydanticOutputParser(pydantic_object=Soybean_Q_3)\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Answer the user query. Wrap the output in `json` tags\\n{format_instructions}\",\n",
    "        ),\n",
    "        (\n",
    "            \"human\", \n",
    "            \"\"\"\n",
    "            You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "            Generate multiple search queries related to:{query}.\n",
    "            \"\"\"\n",
    "        ),\n",
    "    ]\n",
    ").partial(format_instructions=parser.get_format_instructions())\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.6,\n",
    "    model=\"glm-4-0520\",\n",
    "    openai_api_key=\"661a7aa0aeb8ca129eb4647461123230.bl9w581QKpnMfBvs\",\n",
    "    openai_api_base=\"https://open.bigmodel.cn/api/paas/v4/\"\n",
    ")\n",
    "rag_chain = (\n",
    "    {\"query\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | llm\n",
    "    | parser \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0888e1ed-4699-48fb-bcf9-14ea189788c7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-05T08:03:10.758566Z",
     "iopub.status.busy": "2024-07-05T08:03:10.758327Z",
     "iopub.status.idle": "2024-07-05T08:03:33.007699Z",
     "shell.execute_reply": "2024-07-05T08:03:33.007082Z",
     "shell.execute_reply.started": "2024-07-05T08:03:10.758549Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'source', 'page', 'question', 'ground_truth', 'context'])\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain.schema import Document\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm, trange\n",
    "from retiever_eval_list import get_result_retrieva, get_retriever_res_list\n",
    "import os\n",
    "import shutil\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "df = pd.read_excel('soybean_q_gt_609.xlsx', sheet_name='Sheet1')\n",
    "\n",
    "# 修改\n",
    "folder_path = 'Result/a_rag_fusion/'\n",
    "retriever_filename = \"Result/a_rag_fusion/retriever_result.json\"\n",
    "save_info_result_filename = \"Result/a_rag_fusion/save_info_result.json\"\n",
    "top_k = 5\n",
    "s_index = 3\n",
    "\n",
    "model_name = '/mnt/workspace/.cache/modelscope/hub/maple77/zpoint_large_embedding_zh'\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': True}\n",
    "hf = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "vectorstore = Chroma(persist_directory=\"soybean_db2\", embedding_function=hf)\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": top_k}\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/mnt/workspace/.cache/modelscope/hub/Xorbits/bge-reranker-base')\n",
    "rerank_model = AutoModelForSequenceClassification.from_pretrained('/mnt/workspace/.cache/modelscope/hub/Xorbits/bge-reranker-base')\n",
    "\n",
    "\n",
    "def create_folder_if_not_exists(folder_path):\n",
    "    # 检查文件夹是否存在\n",
    "    if os.path.exists(folder_path):\n",
    "        # 如果存在，则删除原文件夹及其中内容\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"Folder '{folder_path}' existed and has been removed.\")\n",
    "    # 创建新文件夹\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder '{folder_path}' has been created.\")\n",
    "\n",
    "# 打印DataFrame的内容\n",
    "column_lists = {col: df[col].tolist() for col in df.columns}\n",
    "print(column_lists.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d438ce08-4c26-4abc-ab10-814236acf7b7",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-05T08:19:50.373342Z",
     "iopub.status.busy": "2024-07-05T08:19:50.373002Z",
     "iopub.status.idle": "2024-07-05T08:26:18.888053Z",
     "shell.execute_reply": "2024-07-05T08:26:18.887445Z",
     "shell.execute_reply.started": "2024-07-05T08:19:50.373324Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get retriever result: 100%|██████████| 3/3 [06:28<00:00, 129.50s/it]\n"
     ]
    }
   ],
   "source": [
    "retriever_result = []\n",
    "for tmp_q in tqdm(range(len(column_lists['question'][:s_index])), desc='Get retriever result'):\n",
    "    # print(tmp_q)\n",
    "    main_query = column_lists['question'][tmp_q]\n",
    "    sub_query = rag_chain.invoke({\"query\": main_query})\n",
    "    retriever_result.append([retriever.invoke(main_query) , retriever.invoke(sub_query.question1) , retriever.invoke(sub_query.question2) , retriever.invoke(sub_query.question3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23de2823-ef91-4bfe-9756-428ef93b582f",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-05T08:37:33.298171Z",
     "iopub.status.busy": "2024-07-05T08:37:33.297792Z",
     "iopub.status.idle": "2024-07-05T08:37:33.303152Z",
     "shell.execute_reply": "2024-07-05T08:37:33.302631Z",
     "shell.execute_reply.started": "2024-07-05T08:37:33.298152Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "reciprocal_result = []\n",
    "for tmp in retriever_result:\n",
    "    reciprocal_result.append(reciprocal_rank_fusion(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2204b66a-e5be-4fe9-aa38-6b9175e21d96",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-05T08:41:33.802096Z",
     "iopub.status.busy": "2024-07-05T08:41:33.801762Z",
     "iopub.status.idle": "2024-07-05T08:41:33.806776Z",
     "shell.execute_reply": "2024-07-05T08:41:33.806180Z",
     "shell.execute_reply.started": "2024-07-05T08:41:33.802077Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'Result/a_rag_fusion/' has been created.\n",
      "完成!\n"
     ]
    }
   ],
   "source": [
    "col_id = column_lists['id'][:s_index]\n",
    "rerank_result = get_result_retrieva(col_id, reciprocal_result, topk=top_k)\n",
    "# 用你想要的路径替换'your_folder_path'\n",
    "create_folder_if_not_exists(folder_path)\n",
    "# 对结果进行保存\n",
    "# 指定你想要保存的文件名\n",
    "# 使用json.dump()将字典保存为json文件\n",
    "with open(retriever_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(rerank_result, f, ensure_ascii=False, indent=4)\n",
    "print('完成!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "833cc28c-05b7-419f-8f08-a940f65be2b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T08:49:25.720974Z",
     "iopub.status.busy": "2024-07-05T08:49:25.720533Z",
     "iopub.status.idle": "2024-07-05T08:49:25.727412Z",
     "shell.execute_reply": "2024-07-05T08:49:25.726857Z",
     "shell.execute_reply.started": "2024-07-05T08:49:25.720939Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完成!\n"
     ]
    }
   ],
   "source": [
    "# dict_keys(['id', 'source', 'page', 'question', 'ground_truth', 'context'])\n",
    "# column_lists['context'][:s_index]\n",
    "save_info_result = {}\n",
    "save_info_result['id'] = column_lists['id'][:s_index]\n",
    "save_info_result['source'] = column_lists['source'][:s_index]\n",
    "save_info_result['page'] = column_lists['page'][:s_index]\n",
    "save_info_result['question'] = column_lists['question'][:s_index]\n",
    "save_info_result['ground_truth'] = column_lists['ground_truth'][:s_index]\n",
    "save_info_result['context'] = column_lists['context'][:s_index]\n",
    "save_info_result['retriever_result_list'] = get_retriever_res_list(reciprocal_result, top_k)\n",
    "\n",
    "# 使用json.dump()将字典保存为json文件\n",
    "with open(save_info_result_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(save_info_result, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print('完成!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
