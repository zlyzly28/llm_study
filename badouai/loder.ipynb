{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c347b0-4bc4-4454-8c07-bc0821084e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\"\"\"\n",
    "数据加载\n",
    "\"\"\"\n",
    "\n",
    "class DataGenerator:\n",
    "    def __init__(self, data_path, config):\n",
    "        self.config = config\n",
    "        self.path = data_path\n",
    "        self.index_to_label = {0: '家居', 1: '房产', 2: '股票', 3: '社会', 4: '文化',\n",
    "                               5: '国际', 6: '教育', 7: '军事', 8: '彩票', 9: '旅游',\n",
    "                               10: '体育', 11: '科技', 12: '汽车', 13: '健康',\n",
    "                               14: '娱乐', 15: '财经', 16: '时尚', 17: '游戏'}\n",
    "        self.label_to_index = dict((y, x) for x, y in self.index_to_label.items())\n",
    "        self.config[\"class_num\"] = len(self.index_to_label)\n",
    "        if self.config[\"model_type\"] == \"bert\":\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(config[\"pretrain_model_path\"])\n",
    "        self.vocab = load_vocab(config[\"vocab_path\"])\n",
    "        self.config[\"vocab_size\"] = len(self.vocab)\n",
    "        self.load()\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        self.data = []\n",
    "        with open(self.path, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                line = json.loads(line)\n",
    "                tag = line[\"tag\"]\n",
    "                label = self.label_to_index[tag]\n",
    "                title = line[\"title\"]\n",
    "                if self.config[\"model_type\"] == \"bert\":\n",
    "                    input_id = self.tokenizer.encode(title, max_length=self.config[\"max_length\"], pad_to_max_length=True)\n",
    "                else:\n",
    "                    input_id = self.encode_sentence(title)\n",
    "                input_id = torch.LongTensor(input_id)\n",
    "                label_index = torch.LongTensor([label])\n",
    "                self.data.append([input_id, label_index])\n",
    "        return\n",
    "\n",
    "    def encode_sentence(self, text):\n",
    "        input_id = []\n",
    "        for char in text:\n",
    "            input_id.append(self.vocab.get(char, self.vocab[\"[UNK]\"]))\n",
    "        input_id = self.padding(input_id)\n",
    "        return input_id\n",
    "\n",
    "    #补齐或截断输入的序列，使其可以在一个batch内运算\n",
    "    def padding(self, input_id):\n",
    "        input_id = input_id[:self.config[\"max_length\"]]\n",
    "        input_id += [0] * (self.config[\"max_length\"] - len(input_id))\n",
    "        return input_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "def load_vocab(vocab_path):\n",
    "    token_dict = {}\n",
    "    with open(vocab_path, encoding=\"utf8\") as f:\n",
    "        for index, line in enumerate(f):\n",
    "            token = line.strip()\n",
    "            token_dict[token] = index + 1  #0留给padding位置，所以从1开始\n",
    "    return token_dict\n",
    "\n",
    "\n",
    "#用torch自带的DataLoader类封装数据\n",
    "def load_data(data_path, config, shuffle=True):\n",
    "    dg = DataGenerator(data_path, config)\n",
    "    dl = DataLoader(dg, batch_size=config[\"batch_size\"], shuffle=shuffle)\n",
    "    return dl\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from config import Config\n",
    "    dg = DataGenerator(\"valid_tag_news.json\", Config)\n",
    "    print(dg[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad7ebd-562f-47a9-a549-406c9fec69db",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "def read_csv_to_dict(file_path):\n",
    "    data = []\n",
    "    with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            # 跳过空行\n",
    "\n",
    "            if not row or row[0] == 'label':\n",
    "                continue\n",
    "            # 检查每行是否至少包含两列\n",
    "            label, content = row[0], row[1]\n",
    "            data.append({\"Label\":int(label), \"Content\": content})\n",
    "\n",
    "    random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "# 使用方法\n",
    "file_path = '文本分类练习.csv'  # 替换为实际的文件路径\n",
    "data_dicts = read_csv_to_dict(file_path)\n",
    "print(data_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15599a04-c091-4ac7-b35c-afbb02712555",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-28T09:43:37.925867Z",
     "iopub.status.busy": "2024-07-28T09:43:37.925528Z",
     "iopub.status.idle": "2024-07-28T09:43:37.935654Z",
     "shell.execute_reply": "2024-07-28T09:43:37.935195Z",
     "shell.execute_reply.started": "2024-07-28T09:43:37.925850Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = []\n",
    "for i in data_dicts:\n",
    "    s.append(len(i['Content']))\n",
    "np.percentile(s, 95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6600c5ba-114d-4d0d-98fb-f4a0acb3eba3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T09:37:14.512782Z",
     "iopub.status.busy": "2024-07-28T09:37:14.512443Z",
     "iopub.status.idle": "2024-07-28T09:37:14.560652Z",
     "shell.execute_reply": "2024-07-28T09:37:14.560205Z",
     "shell.execute_reply.started": "2024-07-28T09:37:14.512765Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "def split_and_save_json(data_list, output_file_80, output_file_20):\n",
    "    # 随机打乱列表以确保数据的随机性\n",
    "    random.shuffle(data_list)\n",
    "\n",
    "    # 计算分割点\n",
    "    split_point = int(len(data_list) * 0.8)\n",
    "\n",
    "    # 分割数据\n",
    "    train_data = data_list[:split_point]\n",
    "    test_data = data_list[split_point:]\n",
    "\n",
    "    # 保存数据到JSON文件\n",
    "    with open(output_file_80, 'w', encoding='utf-8') as f_train:\n",
    "        json.dump(train_data, f_train, ensure_ascii=False, indent=4)\n",
    "\n",
    "    with open(output_file_20, 'w', encoding='utf-8') as f_test:\n",
    "        json.dump(test_data, f_test, ensure_ascii=False, indent=4)\n",
    "\n",
    "# 假设您已经有了一个字典列表\n",
    "# 指定输出文件路径\n",
    "output_file_80 = 'train_data.json'\n",
    "output_file_20 = 'test_data.json'\n",
    "\n",
    "# 调用函数\n",
    "split_and_save_json(data_dicts, output_file_80, output_file_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f512acc-85eb-4353-949b-511700f18844",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-28T10:14:02.823785Z",
     "iopub.status.busy": "2024-07-28T10:14:02.823428Z",
     "iopub.status.idle": "2024-07-28T10:14:03.365008Z",
     "shell.execute_reply": "2024-07-28T10:14:03.364500Z",
     "shell.execute_reply.started": "2024-07-28T10:14:02.823767Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 101, 4157,  749, 2523,  719, 2798, 6843, 3341, 8024, 5445,  684, 4157,\n",
      "        4638, 4156, 6996, 7481, 1789, 4638,  679, 2768, 3416, 2094,  100,  100,\n",
      "        2345, 6397, 8013,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0]), tensor([0])]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\"\"\"\n",
    "数据加载\n",
    "\"\"\"\n",
    "\n",
    "class DataGenerator2:\n",
    "    def __init__(self, data_path, config):\n",
    "        self.config = config\n",
    "        self.path = data_path\n",
    "        self.index_to_label = {0: '负向', 1: '正向'}\n",
    "        self.label_to_index = dict((y, x) for x, y in self.index_to_label.items())\n",
    "        self.config[\"class_num\"] = len(self.index_to_label)\n",
    "        if self.config[\"model_type\"] == \"bert\":\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(config[\"pretrain_model_path\"])\n",
    "        self.vocab = load_vocab(config[\"vocab_path\"])\n",
    "        self.config[\"vocab_size\"] = len(self.vocab)\n",
    "        self.load()\n",
    "\n",
    "\n",
    "    def load(self):\n",
    "        self.data = []\n",
    "        with open(self.path, encoding=\"utf8\") as f:\n",
    "            data = json.load(f)\n",
    "            # print(data)\n",
    "            for line in data:\n",
    "   \n",
    "                label = line[\"Label\"]\n",
    "                title = line[\"Content\"]\n",
    "                if self.config[\"model_type\"] == \"bert\":\n",
    "                    input_id = self.tokenizer.encode(title, max_length=self.config[\"max_length\"], padding='max_length', truncation=True,)\n",
    "                else:\n",
    "                    input_id = self.encode_sentence(title)\n",
    "                input_id = torch.LongTensor(input_id)\n",
    "                label_index = torch.LongTensor([label])\n",
    "                self.data.append([input_id, label_index])\n",
    "        return\n",
    "\n",
    "    def encode_sentence(self, text):\n",
    "        input_id = []\n",
    "        for char in text:\n",
    "            input_id.append(self.vocab.get(char, self.vocab[\"[UNK]\"]))\n",
    "        input_id = self.padding(input_id)\n",
    "        return input_id\n",
    "\n",
    "    #补齐或截断输入的序列，使其可以在一个batch内运算\n",
    "    def padding(self, input_id):\n",
    "        input_id = input_id[:self.config[\"max_length\"]]\n",
    "        input_id += [0] * (self.config[\"max_length\"] - len(input_id))\n",
    "        return input_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "def load_vocab(vocab_path):\n",
    "    token_dict = {}\n",
    "    with open(vocab_path, encoding=\"utf8\") as f:\n",
    "        for index, line in enumerate(f):\n",
    "            token = line.strip()\n",
    "            token_dict[token] = index + 1  #0留给padding位置，所以从1开始\n",
    "    return token_dict\n",
    "\n",
    "def load_json(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "#用torch自带的DataLoader类封装数据\n",
    "def load_data(data_path, config, shuffle=True):\n",
    "    dg = DataGenerator2(data_path, config)\n",
    "    dl = DataLoader(dg, batch_size=config[\"batch_size\"], shuffle=shuffle)\n",
    "    return dl\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from config import Config\n",
    "    dg = DataGenerator2(\"data/test_data.json\", Config)\n",
    "    print(dg[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
