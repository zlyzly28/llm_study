{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba190654-489d-4ff0-801f-587e87099290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install calflops\n",
    "!pip install torchsummary, torchsummaryX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3b4c7c1-c6e6-4116-9f45-c9c0b06ccba1",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2024-07-28T15:44:46.114784Z",
     "iopub.status.busy": "2024-07-28T15:44:46.113958Z",
     "iopub.status.idle": "2024-07-28T15:44:52.924519Z",
     "shell.execute_reply": "2024-07-28T15:44:52.923980Z",
     "shell.execute_reply.started": "2024-07-28T15:44:46.114715Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  102.27 M\n",
      "fwd MACs:                                                               8.43 GMACs\n",
      "fwd FLOPs:                                                              16.88 GFLOPS\n",
      "fwd+bwd MACs:                                                           25.3 GMACs\n",
      "fwd+bwd FLOPs:                                                          50.64 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "BertModel(\n",
      "  102.27 M = 100% Params, 8.43 GMACs = 100% MACs, 16.88 GFLOPS = 100% FLOPs\n",
      "  (embeddings): BertEmbeddings(\n",
      "    16.62 M = 16.25% Params, 0 MACs = 0% MACs, 380.16 KFLOPS = 0% FLOPs\n",
      "    (word_embeddings): Embedding(16.23 M = 15.87% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 21128, 768, padding_idx=1)\n",
      "    (position_embeddings): Embedding(393.22 K = 0.38% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 512, 768)\n",
      "    (token_type_embeddings): Embedding(1.54 K = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 2, 768)\n",
      "    (LayerNorm): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 380.16 KFLOPS = 0% FLOPs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    85.05 M = 83.17% Params, 8.43 GMACs = 99.94% MACs, 16.87 GFLOPS = 99.93% FLOPs\n",
      "    (layer): ModuleList(\n",
      "      (0-11): 12 x BertLayer(\n",
      "        7.09 M = 6.93% Params, 702.38 MMACs = 8.33% MACs, 1.41 GFLOPS = 8.33% FLOPs\n",
      "        (attention): BertAttention(\n",
      "          2.36 M = 2.31% Params, 235.24 MMACs = 2.79% MACs, 470.88 MFLOPS = 2.79% FLOPs\n",
      "          (self): BertSelfAttention(\n",
      "            1.77 M = 1.73% Params, 176.85 MMACs = 2.1% MACs, 353.71 MFLOPS = 2.1% FLOPs\n",
      "            (query): Linear(590.59 K = 0.58% Params, 58.39 MMACs = 0.69% MACs, 116.79 MFLOPS = 0.69% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(590.59 K = 0.58% Params, 58.39 MMACs = 0.69% MACs, 116.79 MFLOPS = 0.69% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(590.59 K = 0.58% Params, 58.39 MMACs = 0.69% MACs, 116.79 MFLOPS = 0.69% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            592.13 K = 0.58% Params, 58.39 MMACs = 0.69% MACs, 117.17 MFLOPS = 0.69% FLOPs\n",
      "            (dense): Linear(590.59 K = 0.58% Params, 58.39 MMACs = 0.69% MACs, 116.79 MFLOPS = 0.69% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 380.16 KFLOPS = 0% FLOPs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          2.36 M = 2.31% Params, 233.57 MMACs = 2.77% MACs, 467.41 MFLOPS = 2.77% FLOPs\n",
      "          (dense): Linear(2.36 M = 2.31% Params, 233.57 MMACs = 2.77% MACs, 467.14 MFLOPS = 2.77% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 270.34 KFLOPS = 0% FLOPs)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          2.36 M = 2.31% Params, 233.57 MMACs = 2.77% MACs, 467.52 MFLOPS = 2.77% FLOPs\n",
      "          (dense): Linear(2.36 M = 2.31% Params, 233.57 MMACs = 2.77% MACs, 467.14 MFLOPS = 2.77% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 380.16 KFLOPS = 0% FLOPs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    590.59 K = 0.58% Params, 5.31 MMACs = 0.06% MACs, 10.62 MFLOPS = 0.06% FLOPs\n",
      "    (dense): Linear(590.59 K = 0.58% Params, 5.31 MMACs = 0.06% MACs, 10.62 MFLOPS = 0.06% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "  )\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('16.88 GFLOPS', '8.43 GMACs', '102.27 M')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from calflops import calculate_flops\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# 模型和分词器\n",
    "batch_size = 1\n",
    "max_seq_length = 128\n",
    "model_save = \"/mnt/workspace/.cache/modelscope/langboat/mengzi-bert-base\"\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModel.from_pretrained(model_save)\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_save)\n",
    "\n",
    "# 文本\n",
    "text = \"这是一段示例文本。\"\n",
    "\n",
    "# 编码文本\n",
    "inputs = tokenizer(text,\n",
    "                   add_special_tokens=True,\n",
    "                   return_attention_mask=True,\n",
    "                   padding=True,\n",
    "                   truncation=\"longest_first\",\n",
    "                   max_length=max_seq_length,\n",
    "                   return_tensors=\"pt\")  # 返回PyTorch张量\n",
    "\n",
    "# 计算FLOPs\n",
    "calculate_flops(model=model,kwargs=inputs,print_results=True)\n",
    "\n",
    "# print(\"Bert FLOPs: %s   MACs: %s   Params: %s\" % (flops, macs, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c886c67-3813-4ecf-ae7f-39243efbff3c",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-28T15:53:27.250286Z",
     "iopub.status.busy": "2024-07-28T15:53:27.249939Z",
     "iopub.status.idle": "2024-07-28T15:53:27.386072Z",
     "shell.execute_reply": "2024-07-28T15:53:27.385475Z",
     "shell.execute_reply.started": "2024-07-28T15:53:27.250265Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "from config import Config\n",
    "from model import TorchModel, choose_optimizer\n",
    "from evaluate import Evaluator\n",
    "from loader import load_data2\n",
    "#[DEBUG, INFO, WARNING, ERROR, CRITICAL]\n",
    "logging.basicConfig(level=logging.INFO, format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "seed = Config[\"seed\"] \n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\"\"\"\n",
    "模型训练主程序\n",
    "\"\"\"\n",
    "config = Config\n",
    "Config[\"num_layers\"] = 2\n",
    "Config[\"model_type\"] = \"rnn\"\n",
    "#创建保存模型的目录\n",
    "if not os.path.isdir(config[\"model_path\"]):\n",
    "    os.mkdir(config[\"model_path\"])\n",
    "#加载训练数据\n",
    "train_data = load_data2(config[\"train_data_path\"], config)\n",
    "#加载模型\n",
    "model = TorchModel(config)\n",
    "# 标识是否使用gpu\n",
    "cuda_flag = torch.cuda.is_available()\n",
    "if cuda_flag:\n",
    "    logger.info(\"gpu可以使用，迁移模型至gpu\")\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ab2ba-2c0f-4d0d-9917-e0ea18e9ea31",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torchsummaryX import summary as summaryX\n",
    "# 使用torchsummaryX库打印模型概要\n",
    "# 注意：这里我们使用一个示例输入来调用summaryX\n",
    "example_input = torch.randint(0, 4623, (100, 100))\n",
    "summaryX(model, example_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39f0a19d-b9e8-45c1-ac98-b7025861f6b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T15:53:33.772502Z",
     "iopub.status.busy": "2024-07-28T15:53:33.772134Z",
     "iopub.status.idle": "2024-07-28T15:53:33.776103Z",
     "shell.execute_reply": "2024-07-28T15:53:33.775574Z",
     "shell.execute_reply.started": "2024-07-28T15:53:33.772479Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TorchModel(\n",
       "  (embedding): Embedding(4623, 256, padding_idx=0)\n",
       "  (encoder): RNN(256, 256, num_layers=2, batch_first=True)\n",
       "  (classify): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93987b00-cb3e-46a9-a6cd-3d9537b9a9e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torchsummaryX"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
