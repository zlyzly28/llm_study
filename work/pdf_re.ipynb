{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b823cdd-0c03-4f6e-b8f6-fc50c561c4f4",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e0bd2-373d-465a-8715-2350d2a8c9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1eabfae7-04eb-4361-bed8-a3dfaec0572c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-30T12:50:34.160685Z",
     "iopub.status.busy": "2024-07-30T12:50:34.160363Z",
     "iopub.status.idle": "2024-07-30T12:50:34.177284Z",
     "shell.execute_reply": "2024-07-30T12:50:34.176836Z",
     "shell.execute_reply.started": "2024-07-30T12:50:34.160662Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "class DataProcess(object):\n",
    "\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.data = []\n",
    "\n",
    "    # 滑动窗口功能实现，其中fast代表当前遍历句子的index，slow代表每次窗口开始滑动的起点。默认窗口直接滑动的overlap是1个句子。\n",
    "    def SlidingWindow(self, sentences, kernel = 512, stride = 1):\n",
    "        sz = len(sentences)\n",
    "        cur = \"\"\n",
    "        fast = 0\n",
    "        slow = 0\n",
    "        while(fast < len(sentences)):\n",
    "            sentence = sentences[fast]\n",
    "            if(len(cur + sentence) > kernel and (cur + sentence) not in self.data):\n",
    "                self.data.append(cur + sentence + \"。\")\n",
    "                cur = cur[len(sentences[slow] + \"。\"):]\n",
    "                slow = slow + 1\n",
    "            cur = cur + sentence + \"。\"\n",
    "            fast = fast + 1\n",
    "\n",
    "    #  数据过滤，根据当前的文档内容的item划分句子，然后根据max_seq划分文档块。\n",
    "    def Datafilter(self, line, header, pageid, max_seq = 1024):\n",
    "\n",
    "         sz = len(line)\n",
    "         if(sz < 6):\n",
    "             return\n",
    "\n",
    "         if(sz > max_seq):\n",
    "\n",
    "             if(\"■\" in line):\n",
    "                 sentences = line.split(\"■\")\n",
    "             elif(\"•\" in line):\n",
    "                 sentences = line.split(\"•\")\n",
    "             elif(\"\\t\" in line):\n",
    "                 sentences = line.split(\"\\t\")\n",
    "             else:\n",
    "                 sentences = line.split(\"。\")\n",
    "\n",
    "             for subsentence in sentences:\n",
    "                 subsentence = subsentence.replace(\"\\n\", \"\")\n",
    "\n",
    "                 if(len(subsentence) < max_seq and len(subsentence) > 5):\n",
    "                     subsentence = subsentence.replace(\",\", \"\").replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "                     if(subsentence not in self.data):\n",
    "                         self.data.append(subsentence)\n",
    "         else:\n",
    "             line = line.replace(\"\\n\",\"\").replace(\",\", \"\").replace(\"\\t\",\"\")\n",
    "             if(line not in self.data):\n",
    "                 self.data.append(line)\n",
    "\n",
    "    # 提取页头即一级标题\n",
    "    def GetHeader(self, page):\n",
    "        try:\n",
    "            lines = page.extract_words()[::]\n",
    "        except:\n",
    "            return None\n",
    "        if(len(lines) > 0):\n",
    "            for line in lines:\n",
    "                if(\"目录\" in line[\"text\"] or \"..........\" in line[\"text\"]):\n",
    "                    return None\n",
    "                if(line[\"top\"] < 20 and line[\"top\"] > 17):\n",
    "                    return line[\"text\"]\n",
    "            return lines[0][\"text\"]\n",
    "        return None\n",
    "\n",
    "    # 按照每页中块提取内容,并和一级标题进行组合,配合Document 可进行意图识别\n",
    "    def ParseBlock(self, max_seq = 1024):\n",
    "\n",
    "        with pdfplumber.open(self.pdf_path) as pdf:\n",
    "\n",
    "            for i, p in enumerate(pdf.pages):\n",
    "                header = self.GetHeader(p)\n",
    "\n",
    "                if(header == None):\n",
    "                    continue\n",
    "\n",
    "                texts = p.extract_words(use_text_flow=True, extra_attrs = [\"size\"])[::]\n",
    "\n",
    "                squence = \"\"\n",
    "                lastsize = 0\n",
    "\n",
    "                for idx, line in enumerate(texts):\n",
    "                    if(idx <1):\n",
    "                        continue\n",
    "                    if(idx == 1):\n",
    "                        if(line[\"text\"].isdigit()):\n",
    "                            continue\n",
    "                    cursize = line[\"size\"]\n",
    "                    text = line[\"text\"]\n",
    "\n",
    "                    if(text == \"□\" or text == \"•\"):\n",
    "                        continue\n",
    "                    elif(text== \"警告！\" or text == \"注意！\" or text == \"说明！\"):\n",
    "                        if(len(squence) > 0):\n",
    "                            self.Datafilter(squence, header, i, max_seq = max_seq)\n",
    "                        squence = \"\"\n",
    "                    elif(format(lastsize,\".5f\") == format(cursize,\".5f\")):\n",
    "                        if(len(squence)>0):\n",
    "                            squence = squence + text\n",
    "                        else:\n",
    "                            squence = text\n",
    "                    else:\n",
    "                        lastsize = cursize\n",
    "                        if(len(squence) < 15 and len(squence)>0):\n",
    "                            squence = squence + text\n",
    "                        else:\n",
    "                            if(len(squence) > 0):\n",
    "                                self.Datafilter(squence, header, i, max_seq = max_seq)\n",
    "                            squence = text\n",
    "                if(len(squence) > 0):\n",
    "                    self.Datafilter(squence, header, i, max_seq = max_seq)\n",
    "\n",
    "    # 按句号划分文档，然后利用最大长度划分文档块\n",
    "    def ParseOnePageWithRule(self, max_seq = 512, min_len = 6):\n",
    "        for idx, page in enumerate(PdfReader(self.pdf_path).pages):\n",
    "            page_content = \"\"\n",
    "            text = page.extract_text()\n",
    "            words = text.split(\"\\n\")\n",
    "            for idx, word in enumerate(words):\n",
    "                text = word.strip().strip(\"\\n\")\n",
    "                if(\"....................\" in text or \"目录\" in text):\n",
    "                    continue\n",
    "                if(len(text) < 1):\n",
    "                    continue\n",
    "                if(text.isdigit()):\n",
    "                    continue\n",
    "                page_content = page_content + text\n",
    "            if(len(page_content) < min_len):\n",
    "                continue\n",
    "            if(len(page_content) < max_seq):\n",
    "                if(page_content not in self.data):\n",
    "                    self.data.append(page_content)\n",
    "            else:\n",
    "                sentences = page_content.split(\"。\")\n",
    "                cur = \"\"\n",
    "                for idx, sentence in enumerate(sentences):\n",
    "                    if(len(cur + sentence) > max_seq and (cur + sentence) not in self.data):\n",
    "                        self.data.append(cur + sentence)\n",
    "                        cur = sentence\n",
    "                    else:\n",
    "                        cur = cur + sentence\n",
    "    #  滑窗法提取段落\n",
    "    #  1. 把pdf看做一个整体,作为一个字符串\n",
    "    #  2. 利用句号当做分隔符,切分成一个数组\n",
    "    #  3. 利用滑窗法对数组进行滑动, 此处的\n",
    "    def ParseAllPage(self, max_seq = 512, min_len = 6):\n",
    "        all_content = \"\"\n",
    "        for idx, page in enumerate(PdfReader(self.pdf_path).pages):\n",
    "            page_content = \"\"\n",
    "            text = page.extract_text()\n",
    "            words = text.split(\"\\n\")\n",
    "            for idx, word in enumerate(words):\n",
    "                text = word.strip().strip(\"\\n\")\n",
    "                if(\"....................\" in text or \"目录\" in text):\n",
    "                    continue\n",
    "                if(len(text) < 1):\n",
    "                    continue\n",
    "                if(text.isdigit()):\n",
    "                    continue\n",
    "                page_content = page_content + text\n",
    "            if(len(page_content) < min_len):\n",
    "                continue\n",
    "            all_content = all_content + page_content\n",
    "        sentences = all_content.split(\"。\")\n",
    "        self.SlidingWindow(sentences, kernel = max_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7767b116-00da-4331-94d2-40377586e506",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "    # 使用 re.sub() 函数替换所有匹配的 URL 为 \"\"\n",
    "    text_without_urls = re.sub(url_pattern, '', text)\n",
    "    specific_text_pattern = re.compile(r'扫描下方二维码关注公众号|提取码|关注|科学上网|回复关键词|侵权|版权|致谢|引用|LICENSE'\n",
    "                                   r'|组队打卡|任务打卡|组队学习的那些事|学习周期|开源内容|打卡|组队学习|链接')\n",
    "    text_without_urls = re.sub(specific_text_pattern, '', text)\n",
    "    return text_without_urls\n",
    "dp =  DataProcess(pdf_path = \"train_a.pdf\")\n",
    "dp.ParseBlock(max_seq = 1024)\n",
    "# dp.ParseBlock(max_seq = 512)\n",
    "print(dp.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2489635-e5c1-4620-9d02-7ee8733d0888",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "\n",
    "    # 使用 re.sub() 函数替换所有匹配的 URL 为 \"\"\n",
    "    text_without_urls = re.sub(url_pattern, '', text)\n",
    "\n",
    "    return text_without_urls\n",
    "def SlidingWindow(sentences, kernel = 512, stride = 1):\n",
    "    data = []\n",
    "    sz = len(sentences)\n",
    "    cur = \"\"\n",
    "    fast = 0\n",
    "    slow = 0\n",
    "    while(fast < len(sentences)):\n",
    "        sentence = sentences[fast]\n",
    "        if(len(cur + sentence) > kernel and (cur + sentence) not in data):\n",
    "            data.append(remove_urls(cur + sentence + \"。\"))\n",
    "            cur = cur[len(sentences[slow] + \"。\"):]\n",
    "            slow = slow + 1\n",
    "        cur = cur + sentence + \"。\"\n",
    "        fast = fast + 1\n",
    "    return data\n",
    "all_content = \"\"\n",
    "for idx, page in enumerate(PdfReader(\"train_a.pdf\").pages):\n",
    "    page_content = \"\"\n",
    "    text = page.extract_text()\n",
    "    words = text.split(\"\\n\")\n",
    "    for idx, word in enumerate(words):\n",
    "        text = word.strip().strip(\"\\n\")\n",
    "        \n",
    "        if(\"....................\" in text or \"目录\" in text):\n",
    "            continue\n",
    "        if(len(text) < 1):\n",
    "            continue\n",
    "        if(text.isdigit()):\n",
    "            continue\n",
    "        page_content = page_content + text\n",
    "    if(len(page_content) < 6):\n",
    "        continue\n",
    "    all_content = all_content + page_content\n",
    "sentences = all_content.split(\"。\")\n",
    "SlidingWindow(sentences, kernel = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bbbea9a6-8f35-46cb-b013-0812ad2bd439",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-30T13:47:53.403246Z",
     "iopub.status.busy": "2024-07-30T13:47:53.402931Z",
     "iopub.status.idle": "2024-07-30T13:47:53.407209Z",
     "shell.execute_reply": "2024-07-30T13:47:53.406727Z",
     "shell.execute_reply.started": "2024-07-30T13:47:53.403227Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'这是一个包含 URL 的示例：http://example.com 和 https://another-example.org。这是不删除的示例,'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 示例文本\n",
    "text = \"这是一个包含 URL 的示例：http://example.com 和 https://another-example.org。这是不删除的示例,扫描下方二维码关注公众号\"\n",
    "\n",
    "specific_text_pattern = re.compile(r'扫描下方二维码关注公众号|提取码|关注|科学上网|回复关键词|侵权|版权|致谢|引用|LICENSE'\n",
    "                                   r'|组队打卡|任务打卡|组队学习的那些事|学习周期|开源内容|打卡|组队学习|链接')\n",
    "# 替换所有匹配的特定文本为空字符串\n",
    "re.sub(specific_text_pattern, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7b82a-2006-4d6b-907c-68cd397fe07f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
